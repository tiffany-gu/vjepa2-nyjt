{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiffany-gu/vjepa2-nyjt/blob/main/Cross_Modal_Transformer_Fusion_Architecture_(with_V_JEPA_2_%26_CoMotion_Context).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# --- Configuration Parameters ---\n",
        "# These would typically be loaded from a config file or passed as arguments\n",
        "# VISION_FEATURE_DIM: Dimension of features extracted from V-JEPA 2.\n",
        "# V-JEPA 2 ViT-g typically outputs 768-dim features.\n",
        "VISION_FEATURE_DIM = 768\n",
        "\n",
        "# SKELETON_FEATURE_DIM: Dimension of features derived from CoMotion's 3D joint coordinates.\n",
        "# If CoMotion outputs (N_joints * 3) coordinates, this would be that dimension,\n",
        "# or a projected dimension after an initial processing layer.\n",
        "SKELETON_FEATURE_DIM = 256 # Example: 25 joints * 3 coords = 75, then projected to 256\n",
        "\n",
        "FUSION_EMBED_DIM = 512    # Common embedding dimension for fused features\n",
        "NUM_HEADS = 8             # Number of attention heads in the transformer\n",
        "NUM_FUSION_LAYERS = 3     # Number of transformer encoder layers in the fusion module\n",
        "NUM_CLASSES = 10          # Example: Number of action classes in InHARD dataset\n",
        "MAX_SEQUENCE_LENGTH = 16  # Example: Number of frames/time steps in a video clip\n",
        "\n",
        "# --- 1. Vision Feature Encoder (Conceptual V-JEPA 2 Integration) ---\n",
        "# This class conceptually represents the process of taking pre-extracted\n",
        "# V-JEPA 2 features and preparing them for the fusion module.\n",
        "# In a real setup, you would have already run V-JEPA 2 on your video clips\n",
        "# (as per Task 1.4) to get these `video_features`.\n",
        "class VisionEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=VISION_FEATURE_DIM, output_dim=VISION_FEATURE_DIM):\n",
        "        super().__init__()\n",
        "        # This layer might be an identity if features are already in the desired dim,\n",
        "        # or a simple projection if further processing is needed before fusion.\n",
        "        # It represents the 'attentive probe' mentioned in the project for V-JEPA 2 baseline,\n",
        "        # but here it's just a pass-through or light transformation for fusion.\n",
        "        self.processor = nn.Identity() if input_dim == output_dim else nn.Linear(input_dim, output_dim)\n",
        "        print(f\"VisionEncoder: Represents processing of pre-extracted V-JEPA 2 features. \"\n",
        "              f\"Expects input features of dim {input_dim}.\")\n",
        "\n",
        "    def forward(self, video_features):\n",
        "        # video_features shape: (batch_size, sequence_length, VISION_FEATURE_DIM)\n",
        "        # These are assumed to be the frame-level feature embeddings pre-extracted\n",
        "        # from the frozen V-JEPA 2 encoder (Task 1.4).\n",
        "        return self.processor(video_features)\n",
        "\n",
        "# --- 2. Skeleton Feature Encoder (Conceptual CoMotion Integration) ---\n",
        "# This class conceptually represents taking 3D skeleton data from CoMotion\n",
        "# and converting it into a feature representation suitable for fusion.\n",
        "# In a real setup, you would have already run CoMotion on your video clips\n",
        "# (as per Task 1.3) to get `skeleton_data` (e.g., 3D joint coordinates or SMPL params).\n",
        "class SkeletonEncoder(nn.Module):\n",
        "    def __init__(self, input_dim=SKELETON_FEATURE_DIM, output_dim=SKELETON_FEATURE_DIM):\n",
        "        super().__init__()\n",
        "        # This could be a simple linear layer, an MLP, or a small transformer\n",
        "        # to process the raw 3D joint coordinates (or SMPL parameters)\n",
        "        # into a fixed-dimension feature vector per time step.\n",
        "        # For demonstration, we use a linear layer.\n",
        "        self.processor = nn.Linear(input_dim, output_dim)\n",
        "        print(f\"SkeletonEncoder: Represents processing of CoMotion 3D skeleton data. \"\n",
        "              f\"Expects input features of dim {input_dim} (e.g., N_joints * 3).\")\n",
        "\n",
        "    def forward(self, skeleton_data):\n",
        "        # skeleton_data shape: (batch_size, sequence_length, SKELETON_FEATURE_DIM)\n",
        "        # These are assumed to be the processed 3D joint coordinates from CoMotion (Task 1.3).\n",
        "        return self.processor(skeleton_data)\n",
        "\n",
        "# --- 3. Cross-Modal Transformer Fusion Architecture ---\n",
        "# This module takes vision and skeleton features and fuses them using a transformer.\n",
        "# The \"skeleton conditioning\" is achieved by combining the projected vision and\n",
        "# skeleton features before feeding them into the transformer, allowing the\n",
        "# self-attention mechanism to learn relationships between the two modalities.\n",
        "class CrossModalTransformer(nn.Module):\n",
        "    def __init__(self, vision_input_dim, skeleton_input_dim, embed_dim, num_heads, num_layers, max_seq_len):\n",
        "        super().__init__()\n",
        "        # Project vision and skeleton features to a common embedding dimension\n",
        "        self.vision_projection = nn.Linear(vision_input_dim, embed_dim)\n",
        "        self.skeleton_projection = nn.Linear(skeleton_input_dim, embed_dim)\n",
        "\n",
        "        # Token for capturing global context (e.g., [CLS] token in BERT)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "\n",
        "        # Positional encoding to capture temporal information\n",
        "        # +1 for the [CLS] token at the beginning of the sequence\n",
        "        self.positional_encoding = nn.Parameter(torch.randn(1, max_seq_len + 1, embed_dim))\n",
        "\n",
        "        # Transformer Encoder layers\n",
        "        # d_model is the feature dimension for the transformer\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=embed_dim * 4, # Standard practice for feedforward network dimension\n",
        "            batch_first=True               # Input and output tensors are (batch, sequence, feature)\n",
        "        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "\n",
        "        self.norm = nn.LayerNorm(embed_dim) # Layer normalization after transformer output\n",
        "\n",
        "        print(f\"CrossModalTransformer: Fusing features with embed_dim={embed_dim}, \"\n",
        "              f\"num_heads={num_heads}, num_layers={num_layers}.\")\n",
        "\n",
        "    def forward(self, vision_features, skeleton_features):\n",
        "        # vision_features shape: (batch_size, sequence_length, VISION_FEATURE_DIM)\n",
        "        # skeleton_features shape: (batch_size, sequence_length, SKELETON_FEATURE_DIM)\n",
        "\n",
        "        batch_size, seq_len, _ = vision_features.shape\n",
        "\n",
        "        # Project features to the common embedding dimension\n",
        "        vision_proj = self.vision_projection(vision_features)   # (B, S, embed_dim)\n",
        "        skeleton_proj = self.skeleton_projection(skeleton_features) # (B, S, embed_dim)\n",
        "\n",
        "        # \"Skeleton Conditioning\": Combine projected vision and skeleton features.\n",
        "        # A simple yet effective way is element-wise addition, allowing the skeleton\n",
        "        # information to directly modulate the visual features at each time step.\n",
        "        # Alternatively, concatenation could be used, or more complex cross-attention.\n",
        "        combined_features_per_timestep = vision_proj + skeleton_proj # (B, S, embed_dim)\n",
        "\n",
        "        # Add CLS token to the beginning of the sequence\n",
        "        cls_tokens = self.cls_token.expand(batch_size, -1, -1) # (B, 1, embed_dim)\n",
        "        # The input sequence for the transformer: [CLS_token, feature_t1, feature_t2, ...]\n",
        "        input_sequence = torch.cat((cls_tokens, combined_features_per_timestep), dim=1) # (B, S+1, embed_dim)\n",
        "\n",
        "        # Add positional encoding to inject temporal information\n",
        "        # Ensure positional_encoding tensor is appropriately sliced for the current sequence length\n",
        "        if input_sequence.shape[1] > self.positional_encoding.shape[1]:\n",
        "            raise ValueError(\n",
        "                f\"Input sequence length ({input_sequence.shape[1]}) exceeds \"\n",
        "                f\"max_seq_len for positional encoding ({self.positional_encoding.shape[1]}). \"\n",
        "                f\"Adjust MAX_SEQUENCE_LENGTH or positional_encoding size.\"\n",
        "            )\n",
        "        input_sequence = input_sequence + self.positional_encoding[:, :input_sequence.shape[1], :]\n",
        "\n",
        "        # Pass the combined sequence through the transformer encoder\n",
        "        transformer_output = self.transformer_encoder(input_sequence)\n",
        "\n",
        "        # Extract the output corresponding to the [CLS] token.\n",
        "        # This token is designed to aggregate information from the entire sequence\n",
        "        # and serves as the fused representation for classification.\n",
        "        fused_representation = transformer_output[:, 0, :] # Shape: (batch_size, embed_dim)\n",
        "\n",
        "        # Apply final layer normalization to the fused representation\n",
        "        return self.norm(fused_representation)\n",
        "\n",
        "# --- 4. Final Classification Head ---\n",
        "# This takes the fused features and maps them to action probabilities (logits).\n",
        "class ClassificationHead(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "        print(f\"ClassificationHead: Outputting {num_classes} classes from {input_dim}-dim features.\")\n",
        "\n",
        "    def forward(self, fused_features):\n",
        "        # fused_features shape: (batch_size, FUSION_EMBED_DIM)\n",
        "        return self.fc(fused_features) # Returns logits\n",
        "\n",
        "# --- 5. Complete Fusion Model ---\n",
        "# Orchestrates the entire process, from feature encoding to fusion and classification.\n",
        "class FusionModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                 vision_feature_dim=VISION_FEATURE_DIM,\n",
        "                 skeleton_feature_dim=SKELETON_FEATURE_DIM,\n",
        "                 fusion_embed_dim=FUSION_EMBED_DIM,\n",
        "                 num_heads=NUM_HEADS,\n",
        "                 num_fusion_layers=NUM_FUSION_LAYERS,\n",
        "                 num_classes=NUM_CLASSES,\n",
        "                 max_sequence_length=MAX_SEQUENCE_LENGTH):\n",
        "        super().__init__()\n",
        "\n",
        "        # Initialize the conceptual encoders for V-JEPA 2 and CoMotion features\n",
        "        self.vision_encoder = VisionEncoder(input_dim=vision_feature_dim, output_dim=vision_feature_dim)\n",
        "        self.skeleton_encoder = SkeletonEncoder(input_dim=skeleton_feature_dim, output_dim=skeleton_feature_dim)\n",
        "\n",
        "        # Initialize the cross-modal transformer for fusion\n",
        "        self.cross_modal_transformer = CrossModalTransformer(\n",
        "            vision_input_dim=vision_feature_dim,\n",
        "            skeleton_input_dim=skeleton_feature_dim,\n",
        "            embed_dim=fusion_embed_dim,\n",
        "            num_heads=num_heads,\n",
        "            num_layers=num_fusion_layers,\n",
        "            max_seq_len=max_sequence_length\n",
        "        )\n",
        "        # Initialize the classification head\n",
        "        self.classification_head = ClassificationHead(fusion_embed_dim, num_classes)\n",
        "        print(\"FusionModel: All components initialized.\")\n",
        "\n",
        "    def forward(self, video_features, skeleton_data):\n",
        "        # 1. Process (conceptually encode) the pre-extracted features\n",
        "        # These steps represent the input features from Task 1.4 (V-JEPA 2)\n",
        "        # and Task 1.3 (CoMotion).\n",
        "        encoded_vision = self.vision_encoder(video_features)\n",
        "        encoded_skeleton = self.skeleton_encoder(skeleton_data)\n",
        "\n",
        "        # 2. Fuse the encoded features using the cross-modal transformer\n",
        "        fused_representation = self.cross_modal_transformer(encoded_vision, encoded_skeleton)\n",
        "\n",
        "        # 3. Classify the resulting fused representation\n",
        "        logits = self.classification_head(fused_representation)\n",
        "        return logits\n",
        "\n",
        "# --- Example Usage (Conceptual) ---\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"--- Initializing Fusion Model ---\")\n",
        "    model = FusionModel()\n",
        "\n",
        "    # Create dummy input data to simulate features from Task 1.4 (V-JEPA 2)\n",
        "    # and Task 1.3 (CoMotion).\n",
        "    # Shape: (batch_size, sequence_length, feature_dim)\n",
        "    dummy_video_features = torch.randn(2, MAX_SEQUENCE_LENGTH, VISION_FEATURE_DIM)\n",
        "    dummy_skeleton_data = torch.randn(2, MAX_SEQUENCE_LENGTH, SKELETON_FEATURE_DIM)\n",
        "\n",
        "    print(f\"\\nDummy Video Features shape (simulating V-JEPA 2 output): {dummy_video_features.shape}\")\n",
        "    print(f\"Dummy Skeleton Data shape (simulating CoMotion output): {dummy_skeleton_data.shape}\")\n",
        "\n",
        "    # Perform a forward pass through the entire model\n",
        "    print(\"\\n--- Performing Forward Pass ---\")\n",
        "    output_logits = model(dummy_video_features, dummy_skeleton_data)\n",
        "\n",
        "    print(f\"Output Logits shape: {output_logits.shape}\")\n",
        "    print(f\"Example Output Logits (first sample): {output_logits[0].detach().numpy()}\")\n",
        "\n",
        "    # This section outlines the conceptual steps for training the model.\n",
        "    # In a real scenario, you would load your InHARD dataset, define\n",
        "    # a loss function, an optimizer, and run a training loop.\n",
        "    print(\"\\n--- Conceptual Training Loop Outline ---\")\n",
        "    # criterion = nn.CrossEntropyLoss() # For multi-class classification\n",
        "    # optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    #\n",
        "    # # Example of a training loop (assuming you have a PyTorch DataLoader for InHARD)\n",
        "    # num_epochs = 10\n",
        "    # for epoch in range(num_epochs):\n",
        "    #     model.train() # Set model to training mode\n",
        "    #     for batch_idx, (video_data_batch, skeleton_data_batch, labels_batch) in enumerate(dataloader):\n",
        "    #         # Move data to the appropriate device (e.g., GPU)\n",
        "    #         # video_data_batch, skeleton_data_batch, labels_batch = video_data_batch.to(device), skeleton_data_batch.to(device), labels_batch.to(device)\n",
        "    #\n",
        "    #         optimizer.zero_grad() # Clear gradients\n",
        "    #         outputs = model(video_data_batch, skeleton_data_batch) # Forward pass\n",
        "    #         loss = criterion(outputs, labels_batch) # Calculate loss\n",
        "    #         loss.backward() # Backward pass\n",
        "    #         optimizer.step() # Update model parameters\n",
        "    #\n",
        "    #     print(f\"Epoch {epoch+1} Loss: {loss.item()}\")\n",
        "    #\n",
        "    #     # After each epoch, typically evaluate on a validation set\n",
        "    #     # model.eval() # Set model to evaluation mode\n",
        "    #     # with torch.no_grad():\n",
        "    #     #     # ... run evaluation logic ...\n",
        "    #     #     print(f\"Validation Accuracy: {accuracy}\")\n",
        "\n",
        "    print(\"\\nThis conceptual implementation covers Task 2.2: Fusion Module Implementation.\")\n",
        "    print(\"It sets up the cross-modal transformer architecture with skeleton conditioning and the final classification head.\")\n",
        "    print(\"The next step (Task 2.3) would be to integrate this model into a full training pipeline.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "VOX1C--TdYj5"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}